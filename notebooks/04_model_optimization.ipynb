{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e4f046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install tensorflow onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561b3440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d63cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODELS_DIR = Path(\"../data/models\")\n",
    "FACE_MODEL_PATH = MODELS_DIR / \"face_detection\" / \"face_detector.h5\"\n",
    "SOUND_MODEL_PATH = MODELS_DIR / \"sound_classification\" / \"audio_classifier.h5\"\n",
    "\n",
    "OUTPUT_DIR = MODELS_DIR / \"optimized\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a26a49",
   "metadata": {},
   "source": [
    "## TensorFlow Lite Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055e049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tflite(model_path, output_path, quantize=True):\n",
    "    \"\"\"\n",
    "    Convert Keras model to TensorFlow Lite.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the Keras model\n",
    "        output_path: Path for the output TFLite model\n",
    "        quantize: Whether to apply quantization\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    # Create converter\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    \n",
    "    if quantize:\n",
    "        # Apply default optimizations (dynamic range quantization)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        \n",
    "        # For full integer quantization (requires representative dataset)\n",
    "        # converter.target_spec.supported_types = [tf.int8]\n",
    "    \n",
    "    # Convert\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    # Save\n",
    "    with open(output_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    \n",
    "    # Report sizes\n",
    "    original_size = Path(model_path).stat().st_size / 1024 / 1024\n",
    "    optimized_size = len(tflite_model) / 1024 / 1024\n",
    "    \n",
    "    print(f\"Original model size: {original_size:.2f} MB\")\n",
    "    print(f\"Optimized model size: {optimized_size:.2f} MB\")\n",
    "    print(f\"Reduction: {(1 - optimized_size/original_size)*100:.1f}%\")\n",
    "    \n",
    "    return tflite_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69858f0",
   "metadata": {},
   "source": [
    "## Benchmark Inference Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe9a07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_tflite(tflite_model_path, input_shape, num_runs=100):\n",
    "    \"\"\"\n",
    "    Benchmark TFLite model inference speed.\n",
    "    \n",
    "    Args:\n",
    "        tflite_model_path: Path to TFLite model\n",
    "        input_shape: Shape of input tensor\n",
    "        num_runs: Number of inference runs\n",
    "    \"\"\"\n",
    "    # Load TFLite model\n",
    "    interpreter = tf.lite.Interpreter(model_path=str(tflite_model_path))\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    # Create random input\n",
    "    input_data = np.random.random(input_shape).astype(np.float32)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "    \n",
    "    # Benchmark\n",
    "    start_time = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    avg_time = (end_time - start_time) / num_runs * 1000\n",
    "    fps = 1000 / avg_time\n",
    "    \n",
    "    print(f\"Average inference time: {avg_time:.2f} ms\")\n",
    "    print(f\"Throughput: {fps:.1f} FPS\")\n",
    "    \n",
    "    return avg_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7542965a",
   "metadata": {},
   "source": [
    "## Full Integer Quantization\n",
    "\n",
    "For maximum performance on Raspberry Pi, use full integer quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4669a003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_int8(model_path, output_path, representative_data_gen):\n",
    "    \"\"\"\n",
    "    Convert model to fully quantized INT8 TFLite.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to Keras model\n",
    "        output_path: Path for output TFLite model\n",
    "        representative_data_gen: Generator yielding representative samples\n",
    "    \"\"\"\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    \n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.representative_dataset = representative_data_gen\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.int8\n",
    "    converter.inference_output_type = tf.int8\n",
    "    \n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    with open(output_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    \n",
    "    print(f\"INT8 model saved to: {output_path}\")\n",
    "    return tflite_model"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
