{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea3e649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install librosa soundfile tensorflow scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78e070a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86702ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "ESC50_PATH = Path(\"../ESC-50-master\")\n",
    "AUDIO_PATH = ESC50_PATH / \"audio\"\n",
    "META_PATH = ESC50_PATH / \"meta\" / \"esc50.csv\"\n",
    "MODEL_OUTPUT_DIR = Path(\"../data/models/sound_classification\")\n",
    "\n",
    "# Audio parameters\n",
    "SAMPLE_RATE = 22050\n",
    "DURATION = 5  # seconds\n",
    "N_MFCC = 40\n",
    "N_MELS = 128\n",
    "\n",
    "# Security-relevant classes\n",
    "SECURITY_CLASSES = [\n",
    "    'glass_breaking',\n",
    "    'door_wood_knock',\n",
    "    'dog',\n",
    "    'siren',\n",
    "    'crying_baby',\n",
    "    'footsteps',\n",
    "    'car_horn',\n",
    "    'clock_alarm'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865d93b1",
   "metadata": {},
   "source": [
    "## Load ESC-50 Dataset Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f7492b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "metadata = pd.read_csv(META_PATH)\n",
    "print(f\"Total samples: {len(metadata)}\")\n",
    "print(f\"\\nCategories:\\n{metadata['category'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6d7557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for security-relevant classes\n",
    "security_data = metadata[metadata['category'].isin(SECURITY_CLASSES)]\n",
    "print(f\"\\nSecurity-relevant samples: {len(security_data)}\")\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(security_data['category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62c4b39",
   "metadata": {},
   "source": [
    "## Audio Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598f10e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(audio_path, sr=SAMPLE_RATE):\n",
    "    \"\"\"Extract MFCC and Mel spectrogram features from audio.\"\"\"\n",
    "    # Load audio\n",
    "    y, sr = librosa.load(audio_path, sr=sr, duration=DURATION)\n",
    "    \n",
    "    # Pad or truncate to fixed length\n",
    "    target_length = sr * DURATION\n",
    "    if len(y) < target_length:\n",
    "        y = np.pad(y, (0, target_length - len(y)))\n",
    "    else:\n",
    "        y = y[:target_length]\n",
    "    \n",
    "    # Extract MFCC\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC)\n",
    "    \n",
    "    # Extract Mel spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=N_MELS)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    \n",
    "    return mfcc, mel_spec_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77868643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Process all audio files and create feature dataset\n",
    "def create_dataset(metadata, audio_path):\n",
    "    \"\"\"Create feature dataset from audio files.\"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for idx, row in metadata.iterrows():\n",
    "        file_path = audio_path / row['filename']\n",
    "        if file_path.exists():\n",
    "            mfcc, mel_spec = extract_features(str(file_path))\n",
    "            features.append(mel_spec)\n",
    "            labels.append(row['category'])\n",
    "    \n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Uncomment to run:\n",
    "# X, y = create_dataset(security_data, AUDIO_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c73b0e",
   "metadata": {},
   "source": [
    "## Build CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e02786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, num_classes):\n",
    "    \"\"\"Build CNN model for audio classification.\"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=input_shape),\n",
    "        \n",
    "        # Conv block 1\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Conv block 2\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Conv block 3\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Classifier\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2e81b8",
   "metadata": {},
   "source": [
    "## Export for Raspberry Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3669a359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_to_tflite(model, output_path):\n",
    "    \"\"\"Export model to TensorFlow Lite for Raspberry Pi.\"\"\"\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    with open(output_path, 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    \n",
    "    print(f\"TFLite model saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
